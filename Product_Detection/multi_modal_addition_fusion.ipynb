{"cells":[{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import os, time, datetime\nimport numpy as np\nimport pandas as pd\nfrom tqdm.notebook import tqdm\nimport random\nimport logging\ntqdm.pandas()\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\n\n#NN Packages\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import TensorDataset, random_split,DataLoader, RandomSampler, SequentialSampler\n\nlogger = logging.getLogger(__name__)\n\n\nif torch.cuda.is_available():    \n\n    # Tell PyTorch to use the GPU.    \n    device = torch.device(\"cuda\")\n\n    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n\n    print('We will use the GPU:', torch.cuda.get_device_name(0))\n\n# If not...\nelse:\n    print('No GPU available, using the CPU instead.')\n    device = torch.device(\"cpu\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def format_time(elapsed):\n    '''\n    Takes a time in seconds and returns a string hh:mm:ss\n    '''\n    # Round to the nearest second.\n    elapsed_rounded = int(round((elapsed)))\n    \n    # Format as hh:mm:ss\n    return str(datetime.timedelta(seconds=elapsed_rounded))\n\nclass SigirPreprocess():\n    \n    def __init__(self, text_data_path):\n        self.text_data_path = text_data_path\n        self.train = None\n        self.dict_code_to_id = {}\n        self.dict_id_to_code = {}\n        self.list_tags = {}\n        self.sentences = []\n        self.labels = []\n        self.text_col = None\n        self.X_test = None\n    def prepare_data(self ):\n        catalog_eng= pd.read_csv(self.text_data_path+\"data/catalog_english_taxonomy.tsv\",sep=\"\\t\")\n        X_train= pd.read_csv(self.text_data_path+\"data/X_train.tsv\",sep=\"\\t\")\n        Y_train= pd.read_csv(self.text_data_path+\"data/Y_train.tsv\",sep=\"\\t\")\n        \n        self.list_tags = list(Y_train['Prdtypecode'].unique())\n        for i,tag in enumerate(self.list_tags):\n            self.dict_code_to_id[tag] = i \n            self.dict_id_to_code[i]=tag\n        print(self.dict_code_to_id)\n            \n        Y_train['labels']=Y_train['Prdtypecode'].map(self.dict_code_to_id)\n        train=pd.merge(left=X_train,right=Y_train,\n               how='left',left_on=['Integer_id','Image_id','Product_id'],\n               right_on=['Integer_id','Image_id','Product_id'])\n        prod_map=pd.Series(catalog_eng['Top level category'].values,\n                           index=catalog_eng['Prdtypecode']).to_dict()\n\n        train['product'] = train['Prdtypecode'].map(prod_map)\n        train['title_len']=train['Title'].progress_apply(lambda x : len(x.split()) if pd.notna(x) else 0)\n        train['desc_len']=train['Description'].progress_apply(lambda x : len(x.split()) if pd.notna(x) else 0)\n        train['title_desc_len']=train['title_len'] + train['desc_len']\n        train.loc[train['Description'].isnull(), 'Description'] = \" \"\n        train['title_desc'] = train['Title'] + \" \" + train['Description']\n        \n        self.train = train\n        \n    def get_sentences(self, text_col, remove_null_rows=False):\n        self.text_col = text_col\n        if remove_null_rows==True:\n            new_train = self.train[self.train[text_col].notnull()]\n\n        else:\n            new_train = self.train.copy()\n            \n        self.sentences = new_train[text_col].values\n        self.labels = new_train['labels'].values\n    \n    def prepare_test(self, text_col):\n        X_test=pd.read_csv(self.text_data_path+\"data/x_test_task1_phase1.tsv\",sep=\"\\t\")\n        X_test.loc[X_test['Description'].isnull(), 'Description'] = \" \"\n        X_test['title_desc'] = X_test['Title'] + \" \" + X_test['Description']\n        self.X_test = X_test\n        self.test_sentences = X_test[text_col].values\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text_col = 'title_desc'\nmax_len = 256\nval_size = 0.1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Preprocess = SigirPreprocess(\"/kaggle/input/textphase1/\")\nPreprocess.prepare_data()\nPreprocess.get_sentences(text_col, True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentences = Preprocess.sentences\nlabels = Preprocess.labels\nprint(\"Total number of sentences:{}, labels:{}\".format(len(sentences), len(labels)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# print('Using Camembert')\n# tokenizer_cam = CamembertTokenizer.from_pretrained('camembert-base', do_lowercase=False)\n# print('Using Flaubert')\n# tokenizer_flau = FlaubertTokenizer.from_pretrained('flaubert/flaubert_base_cased', do_lowercase=False)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#function to prepare input for model training\ndef prep_input(sentences,labels, max_len,tokenizer):\n    input_ids = []\n    attention_masks = []\n\n    # For every sentence...\n    for sent in tqdm(sentences):\n        # `encode_plus` will:\n        #   (1) Tokenize the sentence.\n        #   (2) Prepend the `[CLS]` token to the start.\n        #   (3) Append the `[SEP]` token to the end.\n        #   (4) Map tokens to their IDs.\n        #   (5) Pad or truncate the sentence to `max_length`\n        #   (6) Create attention masks for [PAD] tokens.\n        encoded_dict = tokenizer.encode_plus(\n                            sent,                      # Sentence to encode.\n                            add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n                            max_length = max_len,           # Pad & truncate all sentences.\n                            pad_to_max_length = True,\n                            return_attention_mask = True,   # Construct attn. masks.\n                            return_tensors = 'pt',     # Return pytorch tensors.\n                       )\n\n        # Add the encoded sentence to the list.    \n        input_ids.append(encoded_dict['input_ids'])\n\n        # And its attention mask (simply differentiates padding from non-padding).\n        attention_masks.append(encoded_dict['attention_mask'])\n\n    # Convert the lists into tensors.\n    input_ids = torch.cat(input_ids, dim=0)\n    attention_masks = torch.cat(attention_masks, dim=0)\n    if labels is not None:\n        labels = torch.tensor(labels)\n        return input_ids,attention_masks,labels\n    else:\n        return input_ids,attention_masks\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# input_ids_cam,attention_masks_cam,labels_cam=prep_input(sentences,labels, max_len,tokenizer_cam)\n# # print('Original: ', sentences[0])\n# # print('Token IDs:', input_ids[0]) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# input_ids_flau,attention_masks_flau,labels_flau=prep_input(sentences,labels, max_len,tokenizer_flau)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# tr_inputs_cam, val_inputs_cam, _,_ = train_test_split(input_ids_cam, labels_cam,stratify=labels_cam,\n#                                                             random_state=2020, test_size=val_size)\n# tr_masks_cam, val_masks_cam, _,_ =   train_test_split(attention_masks_cam, labels,stratify=labels,\n#                                              random_state=2020, test_size=val_size)\n\n# tr_inputs_flau, val_inputs_flau, _,_ = train_test_split(input_ids_flau, labels,stratify=labels,\n#                                                             random_state=2020, test_size=val_size)\n# tr_masks_flau, val_masks_flau, _,_ =   train_test_split(attention_masks_flau, labels,stratify=labels_flau,\n#                                              random_state=2020, test_size=val_size)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# torch.save(tr_inputs_cam, \"tr_inputs_cam.pt\")\n# torch.save(val_inputs_cam, \"val_inputs_cam.pt\")\n# torch.save(tr_masks_cam, \"tr_masks_cam.pt\")\n# torch.save(val_masks_cam, \"val_masks_cam.pt\")\n\n# torch.save(tr_inputs_flau, \"tr_inputs_flau.pt\")\n# torch.save(val_inputs_flau, \"val_inputs_flau.pt\")\n# torch.save(tr_masks_flau, \"tr_masks_flau.pt\")\n# torch.save(val_masks_flau, \"val_masks_flau.pt\")\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# !mkdir -p /root/.kaggle/\n# !cp ../input/myjson/kaggle.json /root/.kaggle/\n# !chmod 600 /root/.kaggle/kaggle.json","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# data = '''{\n#   \"title\": \"Multi_modal_input_text\",\n#   \"id\": \"deepbugger/Multi-modal-input-text\",\n#   \"licenses\": [\n#     {\n#       \"name\": \"CC0-1.0\"\n#     }\n#   ]\n# }\n# '''\n# text_file = open(\"/kaggle/working/dataset-metadata.json\", 'w+')\n# n = text_file.write(data)\n# text_file.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# !kaggle datasets create -p /kaggle/working\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text_input='../input/multi-modal-input-text/'\ntr_inputs_cam=torch.load(text_input+\"tr_inputs_cam.pt\")\nval_inputs_cam=torch.load(text_input+\"val_inputs_cam.pt\")\ntr_masks_cam=torch.load( text_input+\"tr_masks_cam.pt\")\nval_masks_cam=torch.load( text_input+\"val_masks_cam.pt\")\n\ntr_inputs_flau=torch.load(text_input+\"tr_inputs_flau.pt\")\nval_inputs_flau=torch.load(text_input+\"val_inputs_flau.pt\")\ntr_masks_flau=torch.load(text_input+\"tr_masks_flau.pt\")\nval_masks_flau=torch.load(text_input+\"val_masks_flau.pt\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install pretrainedmodels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from transformers import CamembertConfig, CamembertTokenizer, CamembertModel, CamembertForSequenceClassification, AdamW\nfrom transformers import FlaubertModel, FlaubertTokenizer,FlaubertForSequenceClassification,AdamW, FlaubertConfig \nfrom transformers.modeling_roberta import RobertaClassificationHead\nfrom transformers.modeling_utils import SequenceSummary","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch.nn import functional as F\nimport torch.nn as nn\nimport pretrainedmodels\n\nclass SEResnext50_32x4d(nn.Module):\n    def __init__(self, pretrained='imagenet'):\n        super(SEResnext50_32x4d, self).__init__()\n        \n        self.base_model = pretrainedmodels.__dict__[\"se_resnext50_32x4d\"](pretrained=None)\n        if pretrained is not None:\n            self.base_model.load_state_dict(\n                torch.load(\"../input/pretrained-model-weights-pytorch/se_resnext50_32x4d-a260b3a4.pth\"\n                )\n            )\n        self.l0 = nn.Linear(2048, 27)\n    \n    def forward(self, image):\n        batch_size, _, _, _ = image.shape\n        \n        x = self.base_model.features(image)\n        x = F.adaptive_avg_pool2d(x, 1).reshape(batch_size, -1)\n        \n        out = self.l0(x)\n\n        return out","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Identity(nn.Module):\n    def __init__(self):\n        super(Identity, self).__init__()\n        \n    def forward(self, x):\n        return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# num_classes=27\n# img_model = SEResnext50_32x4d(pretrained=None)\n# img_model.load_state_dict(torch.load('../input/seresnext2048/best_model.pt'))\n# img_model.cuda()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# img_model.l0=Identity()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# img_model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for params in img_model.parameters():\n#     params.requires_grad=False\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class vec_output_CamembertForSequenceClassification(CamembertModel):\n    config_class = CamembertConfig\n\n    def __init__(self, config):\n        super().__init__(config)\n        self.num_labels = config.num_labels\n\n        self.roberta = CamembertModel(config)\n        self.dense = nn.Linear(256*config.hidden_size, config.hidden_size)\n        self.dropout = nn.Dropout(0.1)\n        self.out_proj = nn.Linear(config.hidden_size, config.num_labels)\n        self.init_weights()\n\n\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,\n        token_type_ids=None,\n        position_ids=None,\n        head_mask=None,\n        inputs_embeds=None,\n        labels=None,\n        output_attentions=None,\n        output_hidden_states=None,\n    ):\n        outputs = self.roberta(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n#             output_attentions=output_attentions,\n#             output_hidden_states=output_hidden_states,\n        )\n        sequence_output = outputs[0] #(B,256,768)\n        x = sequence_output.view(sequence_output.shape[0], 256*768)\n        x = self.dense(x)  # 256*768 -> 768\n        feat= torch.tanh(x) \n        logits = self.out_proj(feat) # 768 -> 27\n        outputs = (logits,) + outputs[2:]\n        \n        return outputs  # (loss), logits, (hidden_states), (attentions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_classes = 27\n\nclass vec_output_FlaubertForSequenceClassification(FlaubertModel):\n    \n    config_class = FlaubertConfig\n    \n\n    def __init__(self, config):\n        super().__init__(config)\n        self.transformer = FlaubertModel(config)\n        self.sequence_summary = SequenceSummary(config)\n        self.init_weights()\n        self.dropout =  torch.nn.Dropout(0.1)\n        self.classifier = torch.nn.Linear(config.hidden_size, num_classes)\n\n\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,\n        langs=None,\n        token_type_ids=None,\n        position_ids=None,\n        lengths=None,\n        cache=None,\n        head_mask=None,\n        inputs_embeds=None,\n        labels=None,\n    ):\n        \n        \n        transformer_outputs = self.transformer(\n            input_ids,\n            attention_mask=attention_mask,\n            langs=langs,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            lengths=lengths,\n            cache=cache,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n        )\n\n        #output = self.dropout(output)\n        output = transformer_outputs[0]\n        vec = output[:,0]\n        \n        \n        #logits\n        dense = self.dropout(vec)\n        \n        #classifier\n        logits = self.classifier(dense)\n        \n        outputs = (logits,) + transformer_outputs[1:]  # Keep new_mems and attention/hidden states if they are here\n       \n        \n        return outputs\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model = vec_output_CamembertForSequenceClassification.from_pretrained(\n#     modelname, # Use the 12-layer BERT model, with an uncased vocab.\n#     num_labels = len(Preprocess.dict_code_to_id), # The number of output labels--2 for binary classification.\n#                     # You can increase this for multi-class tasks.   \n#     output_attentions = False, # Whether the model returns attentions weights.\n#     output_hidden_states = False, # Whether the model returns all hidden-states.\n# )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model_path = '../input/camembert-vec-256m768-10ep/best_model.pt'\n# checkpoint = torch.load(model_path)\n# # model = checkpoint['model']\n# model.load_state_dict(checkpoint)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model.cuda()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for param in model.parameters():\n#     param.requires_grad=False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model.out_proj=Identity()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Image data prep"},{"metadata":{"trusted":true},"cell_type":"code","source":"catalog_eng= pd.read_csv(\"/kaggle/input/textphase1/data/catalog_english_taxonomy.tsv\",sep=\"\\t\")\nX_train= pd.read_csv(\"/kaggle/input/textphase1/data/X_train.tsv\",sep=\"\\t\")\nY_train= pd.read_csv(\"/kaggle/input/textphase1/data/Y_train.tsv\",sep=\"\\t\")\nX_test=pd.read_csv(\"/kaggle/input/textphase1/data/x_test_task1_phase1.tsv\",sep=\"\\t\")\ndict_code_to_id = {}\ndict_id_to_code={}\nlist_tags = list(Y_train['Prdtypecode'].unique())\n\nfor i,tag in enumerate(list_tags):\n    dict_code_to_id[tag] = i \n    dict_id_to_code[i]=tag\nY_train['labels']=Y_train['Prdtypecode'].map(dict_code_to_id)\ntrain=pd.merge(left=X_train,right=Y_train,\n               how='left',left_on=['Integer_id','Image_id','Product_id'],\n               right_on=['Integer_id','Image_id','Product_id'])\nprod_map=pd.Series(catalog_eng['Top level category'].values,index=catalog_eng['Prdtypecode']).to_dict()\ntrain['product']=train['Prdtypecode'].map(prod_map)\n\ndef get_img_path(img_id,prd_id,path):\n    \n    pattern = 'image'+'_'+str(img_id)+'_'+'product'+'_'+str(prd_id)+'.jpg'\n    return path + pattern\ntrain_img = train[['Image_id','Product_id','labels','product']]\n\ntrain_img['image_path']=train_img.progress_apply(lambda x: get_img_path(x['Image_id'],x['Product_id'],\n                                                                path = '/kaggle/input/imagetrain/image_training/'),axis=1)\nX_test['image_path']=X_test.progress_apply(lambda x: get_img_path(x['Image_id'],x['Product_id'],\n                                                    path='/kaggle/input/imagetest/image_test/image_test_task1_phase1/'),axis=1)\ntrain_df, val_df, _, _ = train_test_split(train_img, train_img['labels'],random_state=2020, test_size = 0.1, stratify=train_img['labels'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_size = 224 # for Resnt\n# Applying Transforms to the Data\nfrom torchvision import datasets, models, transforms\n\nimage_transforms = { \n    'train': transforms.Compose([\n        transforms.RandomResizedCrop(size=256, scale=(0.8, 1.0)),\n        transforms.RandomRotation(degrees=15),\n        transforms.RandomHorizontalFlip(),\n        transforms.Resize(size=256),\n        transforms.CenterCrop(size=input_size),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406],\n                             [0.229, 0.224, 0.225])\n    ]),\n    'valid': transforms.Compose([\n        transforms.Resize(size=256),\n        transforms.CenterCrop(size=input_size),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406],\n                             [0.229, 0.224, 0.225])\n    ]),\n    'test': transforms.Compose([\n        transforms.Resize(size=256),\n        transforms.CenterCrop(size=input_size),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406],\n                             [0.229, 0.224, 0.225])\n    ])\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader, Subset\nimport cv2\nfrom PIL import Image\n\nclass FusionDataset(Dataset):\n    \n    def __init__(self,df,inputs_cam,masks_cam,inputs_flau,masks_flau,transform=None,mode='train'):\n        self.df = df\n        self.transform=transform\n        self.mode=mode\n        self.inputs_cam=inputs_cam\n        self.masks_cam=masks_cam\n        self.inputs_flau=inputs_flau\n        self.masks_flau=masks_flau\n         \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self,idx):\n        \n        im_path = self.df.iloc[idx]['image_path']\n        img = cv2.imread(im_path)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        img=Image.fromarray(img)\n        if self.transform is not None:\n            img = self.transform(img)\n        img=img.cuda()\n        input_id_cam=self.inputs_cam[idx].cuda()\n        input_mask_cam=self.masks_cam[idx].cuda()\n        input_id_flau=self.inputs_flau[idx].cuda()\n        input_mask_flau=self.masks_flau[idx].cuda()\n        \n        if self.mode=='test':\n            return img,input_id_cam,input_mask_cam,input_id_flau,input_mask_flau\n        else:\n#             labels = torch.tensor(self.df.iloc[idx]['labels'])\n            labels = torch.tensor(self.df.iloc[idx]['labels']).cuda()             \n\n            return img,input_id_cam,input_mask_cam,input_id_flau,input_mask_flau,labels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 32  \nPreprocess.prepare_test(text_col)\ntest_sentences = Preprocess.test_sentences\nX_test_phase1= Preprocess.X_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print('Using Camembert')\ntokenizer_cam = CamembertTokenizer.from_pretrained('camembert-base', do_lowercase=False)\n# print('Using Flaubert')\ntokenizer_flau = FlaubertTokenizer.from_pretrained('flaubert/flaubert_base_cased', do_lowercase=False)\n\ninput_ids_test_flau,attention_masks_test_flau=prep_input(test_sentences,labels=None, max_len=max_len,tokenizer = tokenizer_flau)\ninput_ids_test_cam,attention_masks_test_cam=prep_input(test_sentences,labels=None, max_len=max_len,tokenizer = tokenizer_cam)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class vector_fusion(nn.Module):\n    \n    def __init__(self):\n        super(vector_fusion, self).__init__()\n        self.img_model = SEResnext50_32x4d(pretrained=None)\n        self.img_model.load_state_dict(torch.load('../input/seresnext2048/best_model.pt'))\n        self.img_model.l0=Identity()\n        for params in self.img_model.parameters():\n            params.requires_grad=False\n\n        self.cam_model= vec_output_CamembertForSequenceClassification.from_pretrained(\n    'camembert-base', # Use the 12-layer BERT model, with an uncased vocab.\n    num_labels = len(Preprocess.dict_code_to_id), # The number of output labels--2 for binary classification.\n                    # You can increase this for multi-class tasks.   \n    output_attentions = False, # Whether the model returns attentions weights.\n    output_hidden_states = False,) # Whether the model returns all hidden-states.\n        \n        \n        cam_model_path = '../input/camembert-vec-256m768-10ep/best_model.pt'\n        checkpoint = torch.load(cam_model_path)\n        # model = checkpoint['model']\n        self.cam_model.load_state_dict(checkpoint)\n        for param in self.cam_model.parameters():\n            param.requires_grad=False\n        self.cam_model.out_proj=Identity()\n        \n        self.flau_model=vec_output_FlaubertForSequenceClassification.from_pretrained(\n        'flaubert/flaubert_base_cased', \n        num_labels = len(Preprocess.dict_code_to_id), \n        output_attentions = False,\n        output_hidden_states = False,)\n        flau_model_path='../input/flaubert-8933/best_model.pt'\n        checkpoint = torch.load(flau_model_path)\n        self.flau_model.load_state_dict(checkpoint)\n        for param in self.flau_model.parameters():\n            param.requires_grad=False\n        self.flau_model.classifier=Identity()\n        \n        \n        #reducing the dimensionality\n        self.reduce_dim=nn.Conv1d(in_channels = 2048 , out_channels = 768 , kernel_size= 1)\n        \n        #output\n        self.out=nn.Linear(768, 27)\n        \n\n        \n        \n    def forward(self,img,input_id_cam,input_mask_cam,input_id_flau,input_mask_flau):\n        \n        cam_emb =self.cam_model(input_id_cam, \n                     token_type_ids=None,               ###### bs * 768  \n                     attention_mask=input_mask_cam)\n        \n        #alignment\n        #cam_emb1 = cam_emb[0]\n        \n        \n        flau_emb =self.flau_model(input_id_flau,  \n                     token_type_ids=None,               ###### bs * 768 \n                     attention_mask=input_mask_flau)\n        \n        #alignment\n        #flau_emb1 = flau_emb[0]\n        \n        #Projecting the image embedding to lower dimension\n        img_emb=self.img_model(img)\n        img_emb=img_emb.view(img_emb.shape[0],img_emb.shape[1],1) \n        img_emb=self.reduce_dim(img_emb)                         \n        img_emb=img_emb.view(img_emb.shape[0],img_emb.shape[1]) ###### bs * 768 \n        \n        #adding\n        fuse= img_emb + cam_emb[0] + flau_emb[0]\n        \n        logits=self.out(fuse)\n        return logits","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = vector_fusion()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.cuda()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset=FusionDataset(train_df,tr_inputs_cam,tr_masks_cam,tr_inputs_flau,tr_masks_flau,\n                            transform=image_transforms['test'])\nval_dataset=FusionDataset(val_df,val_inputs_cam,val_masks_cam,val_inputs_flau,val_masks_flau,\n                          transform=image_transforms['test'])\ntest_dataset=FusionDataset(X_test,input_ids_test_cam,attention_masks_test_cam,input_ids_test_flau,attention_masks_test_flau\n                           ,transform=image_transforms['test'],mode = 'test')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size=64\ntrain_dataloader=DataLoader(train_dataset,batch_size=batch_size,shuffle=True)\nvalidation_dataloader=DataLoader(val_dataset,batch_size=batch_size,shuffle=False)\ntest_dataloader=DataLoader(test_dataset,batch_size=batch_size,shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# tr_inputs, val_inputs, tr_labels, val_labels = train_test_split(input_ids, labels,stratify=labels,\n#                                                             random_state=2020, test_size=val_size)\n# tr_masks, val_masks, u,v =   train_test_split(attention_masks, labels,stratify=labels,\n#                                              random_state=2020, test_size=val_size)\n\n\n# train_dataset=TensorDataset(tr_inputs, tr_masks, tr_labels)\n# val_dataset=TensorDataset(val_inputs, val_masks, val_labels)\n# train_sampler = RandomSampler(train_dataset) \n# valid_sampler = SequentialSampler(val_dataset)\n# from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n\n# # The DataLoader needs to know our batch size for training, so we specify it \n# # here. For fine-tuning BERT on a specific task, the authors recommend a batch \n# # size of 16 or 32.\n# batch_size = 32\n\n# # Create the DataLoaders for our training and validation sets.\n# # We'll take training samples in random order. \n# train_dataloader = DataLoader(\n#             train_dataset,  # The training samples.\n#             sampler = train_sampler, # Select batches randomly\n#             batch_size = batch_size # Trains with this batch size.\n#         )\n\n# # For validation the order doesn't matter, so we'll just read them sequentially.\n# validation_dataloader = DataLoader(\n#             val_dataset, # The validation samples.\n#             sampler = valid_sampler, # Pull out batches sequentially.\n#             batch_size = batch_size # Evaluate with this batch size.\n#         )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optimizer = AdamW(model.parameters(),\n                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n                )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\ncount_parameters(model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from transformers import get_linear_schedule_with_warmup\n\n# Number of training epochs. The BERT authors recommend between 2 and 4. \n# We chose to run for 4, but we'll see later that this may be over-fitting the\n# training data.\nepochs = 6\n\n# Total number of training steps is [number of batches] x [number of epochs]. \n# (Note that this is not the same as the number of training samples).\ntotal_steps = len(train_dataloader) * epochs\n\n# Create the learning rate scheduler.\nscheduler = get_linear_schedule_with_warmup(optimizer, \n                                            num_warmup_steps = 0, # Default value in run_glue.py\n                                            num_training_steps = total_steps)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch.nn as nn\nloss_criterion = nn.CrossEntropyLoss()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def flat_accuracy(preds, labels):\n    pred_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n    return np.sum(pred_flat == labels_flat) / len(labels_flat)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import f1_score\n\nseed_val = 42\n\nrandom.seed(seed_val)\nnp.random.seed(seed_val)\ntorch.manual_seed(seed_val)\ntorch.cuda.manual_seed_all(seed_val)\n\n# We'll store a number of quantities such as training and validation loss, \n# validation accuracy, and timings.\ntraining_stats = []\n\n# Measure the total training time for the whole run.\ntotal_t0 = time.time()\n\n\n# For each epoch...\nfor epoch_i in range(0, epochs):\n    \n    # ========================================\n    #               Training\n    # ========================================\n    \n    # Perform one full pass over the training set.\n\n    print(\"\")\n    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n    print('Training...')\n    \n    #tr and val\n#     vec_output_tr = []\n#     vec_output_val =[]\n\n    # Measure how long the training epoch takes.\n    t0 = time.time()\n\n    # Reset the total loss for this epoch.\n    total_train_loss = 0\n\n    # Put the model into training mode. Don't be mislead--the call to \n    # `train` just changes the *mode*, it doesn't *perform* the training.\n    # `dropout` and `batchnorm` layers behave differently during training\n    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n    best_f1 = 0\n    model.train()\n\n    # For each batch of training data...\n    for step, batch in tqdm(enumerate(train_dataloader)):\n        \n        # Unpack this training batch from our dataloader. \n        #\n        \n        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n        # `to` method.\n        #\n        # `batch` contains three pytorch tensors:\n        #   [0]: input ids \n        #   [1]: attention masks\n        #   [2]: labels \n#         return img,input_id_cam,input_mask_cam,input_id_flau,input_mask_flau\n\n        b_img=batch[0].to(device)\n\n        b_input_id_cam = batch[1].to(device)\n        b_input_mask_cam = batch[2].to(device)\n        b_input_id_flau = batch[3].to(device)\n        b_input_mask_flau = batch[4].to(device)\n\n        b_labels = batch[5].to(device)\n        \n        \n        model.zero_grad()        \n\n        \n        logits = model(b_img,b_input_id_cam ,b_input_mask_cam,b_input_id_flau,b_input_mask_flau)\n                            \n        #Defining the loss\n        loss = loss_criterion(logits, b_labels)\n        \n        #saving the features_tr\n#         vec = vec.detach().cpu().numpy()\n#         vec_output_tr.extend(vec)\n        \n        # Accumulate the training loss over all of the batches so that we can\n        # calculate the average loss at the end. `loss` is a Tensor containing a\n        # single value; the `.item()` function just returns the Python value \n        # from the tensor.\n        total_train_loss += loss.item()\n\n        # Perform a backward pass to calculate the gradients.\n        loss.backward()\n\n        # Clip the norm of the gradients to 1.0.\n        # This is to help prevent the \"exploding gradients\" problem.\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n\n        # Update parameters and take a step using the computed gradient.\n        # The optimizer dictates the \"update rule\"--how the parameters are\n        # modified based on their gradients, the learning rate, etc.\n        optimizer.step()\n\n        # Update the learning rate.\n        scheduler.step()\n        \n        \n        \n\n    # Calculate the average loss over all of the batches.\n    avg_train_loss = total_train_loss / len(train_dataloader)            \n    \n    # Measure how long this epoch took.\n    training_time = format_time(time.time() - t0)\n\n    print(\"\")\n    print(\"  Average training loss: {0:.2f} \".format(avg_train_loss))\n    print(\"  Training epcoh took: {:} \".format(training_time))\n        \n    # ========================================\n    #               Validation\n    # ========================================\n    # After the completion of each training epoch, measure our performance on\n    # our validation set.\n\n    print(\"\")\n    print(\"Running Validation...\")\n\n    t0 = time.time()\n\n    # Put the model in evaluation mode--the dropout layers behave differently\n    # during evaluation.\n    model.eval()\n\n    # Tracking variables \n    total_eval_accuracy = 0\n    total_eval_loss = 0\n    nb_eval_steps = 0\n    predictions=[]\n    true_labels=[]\n    \n\n    # Evaluate data for one epoch\n    for batch in tqdm(validation_dataloader):\n        \n        # Unpack this training batch from our dataloader. \n        #\n        # As we unpack the batch, we'll also copy each tensor to the GPU using \n        # the `to` method.\n        #\n        # `batch` contains three pytorch tensors:\n        #   [0]: input ids \n        #   [1]: attention masks\n        #   [2]: labels \n        b_img=batch[0].to(device)\n\n        b_input_id_cam = batch[1].to(device)\n        b_input_mask_cam = batch[2].to(device)\n        b_input_id_flau = batch[3].to(device)\n        b_input_mask_flau = batch[4].to(device)\n\n        b_labels = batch[5].to(device)\n        \n        \n        # Tell pytorch not to bother with constructing the compute graph during\n        # the forward pass, since this is only needed for backprop (training).\n        with torch.no_grad():       \n        \n\n            # Forward pass, calculate logit predictions.\n            # token_type_ids is the same as the \"segment ids\", which \n            # differentiates sentence 1 and 2 in 2-sentence tasks.\n            # The documentation for this `model` function is here: \n            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n            # Get the \"logits\" output by the model. The \"logits\" are the output\n            # values prior to applying an activation function like the softmax.\n            logits = model(b_img,b_input_id_cam ,b_input_mask_cam,b_input_id_flau,b_input_mask_flau)\n            \n        #new\n        \n        #defining the val loss\n        loss = loss_criterion(logits, b_labels)\n        \n        \n        # Accumulate the validation loss.\n        total_eval_loss += loss.item()\n\n        # Move logits and labels to CPU\n        logits = logits.detach().cpu().numpy()\n\n        # Move logits and labels to CPU\n        predicted_labels=np.argmax(logits,axis=1)\n        predictions.extend(predicted_labels)\n        label_ids = b_labels.to('cpu').numpy()\n        true_labels.extend(label_ids)\n        \n        #saving the features_tr\n#         vec = vec.detach().cpu().numpy()\n#         vec_output_val.extend(vec)\n        \n\n        # Calculate the accuracy for this batch of test sentences, and\n        # accumulate it over all batches.\n        total_eval_accuracy += flat_accuracy(logits, label_ids)\n        \n\n    # Report the final accuracy for this validation run.\n    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n\n    # Calculate the average loss over all of the batches.\n    avg_val_loss = total_eval_loss / len(validation_dataloader)\n    \n    # Measure how long the validation run took.\n    validation_time = format_time(time.time() - t0)\n    \n    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n    print(\"  Validation took: {:}\".format(validation_time))\n    print(\"Validation F1-Score: {}\".format(f1_score(true_labels,predictions,average='macro')))\n    curr_f1=f1_score(true_labels,predictions,average='macro')\n    if curr_f1 > best_f1:\n        best_f1=curr_f1\n        torch.save(model.state_dict(), 'best_model.pt')\n#         np.save('best_vec_train_model_train.npy',vec_output_tr)\n#         np.save('best_vec_val.npy',vec_output_val)\n        \n    # Record all statistics from this epoch.\n#     training_stats.append(\n#         {\n#             'epoch': epoch_i + 1,\n#             'Training Loss': avg_train_loss,\n#             'Valid. Loss': avg_val_loss,\n#             'Valid. Accur.': avg_val_accuracy,\n#             'Training Time': training_time,\n#             'Validation Time': validation_time\n#         }\n#     )\n\nprint(\"\")\nprint(\"Training complete!\")\n\nprint(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ckpt = '../input/vec-fusion-9093/best_model.pt'\nmodel.load_state_dict(torch.load(ckpt))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict_pyt(model, prediction_dataloader):\n    \"\"\"\n    model: pytorch model\n    prediction_dataloader: DataLoader object for which the predictions has to be made.\n    return:\n        predictions:- Direct predicted labels\n        softmax_logits:- logits which are normalized with softmax on output\"\"\"\n    # Put model in evaluation mode\n    model.eval()\n    # Tracking variables \n    predictions = []\n    softmax_logits=[]\n    # Predict \n    \n    for batch in tqdm(prediction_dataloader):\n        \n        # Add batch to GPU\n        b_img=batch[0].to(device)\n        b_input_id_cam = batch[1].to(device)\n        b_input_mask_cam = batch[2].to(device)\n        b_input_id_flau = batch[3].to(device)\n        b_input_mask_flau = batch[4].to(device)\n        \n        \n        # Telling the model not to compute or store gradients, saving memory and \n        # speeding up prediction\n        with torch.no_grad():\n            # Forward pass, calculate logit predictions\n            logits = model(b_img,b_input_id_cam ,b_input_mask_cam,b_input_id_flau,b_input_mask_flau)\n        \n        \n        #find logits\n    #----- Add softmax---     \n        m = nn.Softmax(dim=1)\n    # #     input = torch.randn(2, 3)\n        output = m(logits)\n    #-------#------\n        # Move logits and labels to CPU\n        logits = logits.detach().cpu().numpy()\n        predicted_labels=np.argmax(logits,axis=1)\n        predictions.extend(predicted_labels)\n        softmax_logits.extend(output)\n    print('DONE')\n    return predictions, softmax_logits\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#validation predictions\npredictions_val, softmax_logits_val = predict_pyt(model, validation_dataloader)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"softmax_logits_val = np.array([ten.detach().cpu().numpy() for ten in softmax_logits_val])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.save('predictions_val_add.npy',np.array(predictions_val))\nnp.save('softmax_logits_val_add.npy',softmax_logits_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#test_predictions\n#predictions_test, softmax_logits_test = predict_pyt(model, test_dataloader)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#softmax_logits_test = np.array([ten.detach().cpu().numpy() for ten in softmax_logits_test])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# np.save('predictions_test_9093.npy',np.array(predictions_test))\n# np.save('softmax_logits_test_9093.npy',softmax_logits_test)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}